# -*- coding: utf-8 -*-
"""NLP with Disaster Tweets Model Prediction 09052024

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/nlp-with-disaster-tweets-model-prediction-09052024-da3c8b28-a093-4abe-b7c8-23c06cb12a22.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240905/auto/storage/goog4_request%26X-Goog-Date%3D20240905T232348Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db628d64c80f89707601842df40397445b867ea9d98231f64ed8c29b0e38bab3fd25c4c3e740a19d279ac4293bd4aeb72d5755d043ce1b162e64f2299a52d2066c9f64207407bcd3d120c78de243e0518bf58b102d31f09870f5b448f00a06830119693766de3d47344de6983ff353b36974d2c48e7e8bdfe9d483fc50d26947499512737d219e4d70753b5fa46ceac2b1188cc4be7019fe423a17c6a30bafc1302b43dd4c119987bf1ecf7412f4c103bfaf26be28ab836fe0163f6793d9653bbc00cacbe2b15ff82aa955988115e0a91ebb36eadd05275897136a9c718d4caecc88c2046476207fb3799b81c96a6e9740091135aa9b066055926495816be5880
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'nlp-getting-started:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F17777%2F869809%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240905%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240905T232348Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D360034cf1fab22f1d614144a68c599286b030c1ab7e4ac6a6178bc769227dc7a6432e208449dcfbabe488990326c6636a05c8c3399fda5366c42d4f00c39e89c7099ac870a2990985438ac3d4b114a7268af349c2feb94117d8e34076f0cf33ae966a6758c57e0c04159ef6fad3740bf99857b9f3282ec409220a270bd7e59f7c7882e0e72f1dbbcfbc3f15242a0a9c718164abae8bed25338ead42a2b83455f8226c3b88cbb743633b9d0db5c3bb2670a6e6e103ed0282a1dd3305c230bcf8bc423d47646342703549ecc51bec0fc96a8d36d389341ac3dcfb02bbfaeedc2f96e582afdcfd2c4384ba8e45feb13b89a16b22b087e046dc1acf5823fe987596c,disasters-on-social-media:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F459672%2F865552%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240905%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240905T232348Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0f1c19b58ff3edd7d58efee7f5d46bbc982d0b4396e8c10347d0a4fdf09761cbdbe9ebb61ad3899e60e6c695012230e83957580557778c7d41f2882fe9ad31d289a749978c794b738926f484971b538837b06f40e0eae0f089a7cf5e427cf77061088b1d10e8074d973e2db51be954ae39e5e05d1b764169edcaa7173df14a235f6870c4f631199c7d0220b0fa8b00883c3c1e71d04f2530ab837178af5126dc74beb15e932416f19f124004eee84ab78982dd091a02fda8951d197a9aae6d1d31b52ee197b784afd184aad85089871131b27fcde906bd0338edf06d60a848d4df5ed2fa46d829fd5441ccd39e044700d24fa3ebdb6c6de581804f3dc1c59c82'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

#import library

!pip install pivottablejs
!pip install pandas_profiling
!pip install pygwalker

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
import matplotlib.pyplot as plt
import seaborn as sb
import re
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
import warnings
warnings.filterwarnings("ignore")
from nltk.corpus import stopwords
from collections import Counter, defaultdict
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.metrics import f1_score
import lime
from lime import lime_text
from lime.lime_text import LimeTextExplainer
from pandas_profiling import ProfileReport
import pygwalker as pyg
from pivottablejs import pivot_ui

#Data Exploration

wordnet_lemmatizer = WordNetLemmatizer()

for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')

train_profile = ProfileReport(train, title="train")
train_profile.to_notebook_iframe()

pivot_ui(train)

walker = pyg.walk(train)

test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')

test_profile = ProfileReport(test, title="test")
test_profile.to_notebook_iframe()

pivot_ui(test)

walker = pyg.walk(test)

test_profile = ProfileReport(test, title="test")
test_profile.to_notebook_iframe()

sample_submission=pd.read_csv("/kaggle/input/nlp-getting-started/sample_submission.csv")

sample_submission_profile = ProfileReport(sample_submission, title="sample_submission")
sample_submission_profile.to_notebook_iframe()

pivot_ui(sample_submission)

walker = pyg.walk(sample_submission)

socialmedia_disaster_tweets=pd.read_csv("/kaggle/input/disasters-on-social-media/socialmedia-disaster-tweets-DFE.csv")

socialmedia_disaster_tweets_profile = ProfileReport(socialmedia_disaster_tweets, title="socialmedia_disaster_tweets")
socialmedia_disaster_tweets_profile.to_notebook_iframe()

pivot_ui(socialmedia_disaster_tweets)

walker = pyg.walk(socialmedia_disaster_tweets)

train.info()

#There is a total of 7613 rows(entries) with 4 columns(features) other than id.
#keyword tells about a particular keyword from text which might relate to disaster. It contains 61(0.8%) missing values.
#location tells about where the tweet was sent from. It contains 2533(33.27%) missing values.
#text is the tweet made.¶
#target tells whether the tweet is real disaster(1) or not(0). It has a split of 0 with 4342(57%) and 1 with 3271(43%)

top = train.groupby('keyword')['id'].count()
top = pd.DataFrame({'keyword':top.index,'count':top.values}).sort_values(by=['count']).tail(20)

bottom = train.groupby('keyword')['id'].count()
bottom = pd.DataFrame({'keyword':bottom.index,'count':bottom.values}).sort_values(by=['count']).head(20)

plt.figure(figsize=(12,10))

plt.subplot(211)
barlist = plt.bar(data=top, x = 'keyword',height = 'count',color = 'cadetblue')
plt.xticks(rotation = 20);
plt.ylabel('count')
plt.title('Top 20 unique keywords')
barlist[0].set_color('darkgoldenrod');
barlist[2].set_color('indianred');
barlist[3].set_color('indianred');
barlist[15].set_color('darkgoldenrod');
barlist[9].set_color('darkslategrey');
barlist[18].set_color('darkseagreen');

plt.subplot(212)
barlist = plt.bar(data=bottom, x = 'keyword',height = 'count', color = 'cadetblue');
plt.xticks(rotation = 45);
plt.ylabel('count');
plt.title('Bottom 20 unique keywords')
barlist[14].set_color('darkslategrey');
barlist[10].set_color('darkseagreen');

sb.despine(left = True, bottom  = True)
plt.tight_layout()

print(str(train['keyword'].nunique())+ ' total unique keywords')

#Observations
#fatalities was the highest keyword with around 42 tweets containing keyword. radiation emergency was the least with around 9 tweets containing it.
#From the top20 we can see wrecked and wreckage as different keywords but both mean the same, just tense is different. There are other keywords also present in same format eg: dead, death, annihilation, annihilated, sunk, sinking etc.
#same colors are given to repeated words with different tenses
#Text handling to be done
#Repalce missing with None
#lemmatize and change values with same meaninig into a single value.
#repalce the '%20' in text with some thing else

top = train.groupby('location')['id'].count()
top = pd.DataFrame({'location':top.index,'count':top.values}).sort_values(by=['count']).tail(20)


plt.figure(figsize=(16,6))

barlist = plt.bar(data=top, x = 'location',height = 'count', color = 'cadetblue')
plt.xticks(rotation = 90);
plt.ylabel('count')
plt.title('Top 20 unique locations')

barlist[1].set_color('darksalmon')
barlist[4].set_color('darksalmon')
barlist[3].set_color('peru')
barlist[18].set_color('peru')
barlist[17].set_color('dimgrey')
barlist[19].set_color('dimgrey')

sb.despine(left = True, bottom  = True)

print(str(train['location'].nunique())+ ' total unique locations')

#Observations¶
#3341 unique values in 7631 which is 43.5% of the data.
#Even unique there are few values which have repeated like "United States" and "USA" represent the same. "New York, NY" and "New York" both are same.
#This column can be a country like INDIA or a city in that country Mumbai. Because the tweets can be specific to city or generic to a country.
#Text Handling to be done
#Replace NaN with None
#change the same locations into one eg : 'New York NY', 'New York' both are same.
#Dont not replce city to country eg: 'Mumbai' and 'INDIA' because tweet can be specifict to city or to a country.



#Clean the Code
#From all the observations made it looks lot of cleaning is required. lets list them
#Repalce missing in keyword and location column with None
#In keyword column lemmatize and change values with same meaninig into a single value.
#IN keyword repalce the '%20' in text with some thing else
#change the same locations into one eg : 'New York, NY' and 'New York' both are same.
#tags representing cities, incidents @ notations and http links to handle
#Special characters like ' -, => , .,: ' are also present to handle.
#Special symbols like '\x89UO' are present might be emotes need to be removed

def find_URL(text):
    url = re.compile(r'https?://\S+|www\.\S+')
    return (url.search(text) != None)

def clean_text(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)/bayraklar (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)

    url = re.compile(r'https?://\S+|www\.\S+')
    text = url.sub(r'',text)

    text = text.replace('#',' ')
    text = text.replace('@',' ')
    symbols = re.compile(r'[^A-Za-z0-9 ]')
    text = symbols.sub(r'',text)

    return text

def lemma(text):
    txt1 = wordnet_lemmatizer.lemmatize(text,pos=wordnet.NOUN)
    txt2 = wordnet_lemmatizer.lemmatize(text,pos=wordnet.VERB)
    txt3 = wordnet_lemmatizer.lemmatize(text,pos=wordnet.ADJ)
    if len(txt1) <= len(txt2) and len(txt1) <= len(txt3):
        text = txt1
    elif len(txt2) <= len(txt1) and len(txt2) <= len(txt3):
        text = txt2
    else:
        text = txt3

    return text

#Replace missing values

train['keyword'].fillna('None', inplace=True)
train['location'].fillna('None', inplace=True)

#Replace %20 in the keyword column

train['keyword'] = train['keyword'].str.replace('%20','')

#location column handling

for ind in range(train.shape[0]):
    train.loc[ind,'location'] = train.loc[ind,'location'].split(',')[0]

#Text column handling

for ind in range(train.shape[0]):
    train.loc[ind,'tags_count'] = len(train.loc[ind,'text']) -  len(train.loc[ind,'text'].replace('#',''))
    train.loc[ind,'@_count'] = len(train.loc[ind,'text']) -  len(train.loc[ind,'text'].replace('@',''))
    train.loc[ind,'http_link'] =  find_URL(train.loc[ind,'text'])

train['text'] = train['text'].apply(lambda x: clean_text(x))

train.head(70)

import matplotlib.pyplot as plt
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from wordcloud import WordCloud

target = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')['target']
train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')
test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')
ssub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')

train['hashtags']=train['text'].apply(lambda x:re.findall('#\w*',x))

labels=['Negative','Positive']
no_clusters=2

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd

# Loop through two labels
for c in range(2):
    print('Target:-',labels[c])
    hts = list(train[train['target'] == c]['hashtags'])

    # Flatten list of lists into a single list
    hashes = [h.strip() for ht in hts for h in ht]

    # Join list of hashtags into a string
    string_hash = ' '.join(hashes)

    # Count the frequency of each hashtag
    hash_values = pd.Series(hashes).value_counts()

    # Convert frequency series to a dictionary
    d = hash_values.to_dict()

    # Generate wordcloud from frequency dictionary
    wordcloud = WordCloud(max_font_size=40)
    wordcloud.generate_from_frequencies(frequencies=d)

    # Plot wordcloud/Kelime bulutunu çiz
    plt.figure(figsize=(70, 70))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()

#Model Prediction Algorithm

import pandas as pd
test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')
gt_df = pd.read_csv("../input/disasters-on-social-media/socialmedia-disaster-tweets-DFE.csv")

# select columns 'choose_one' and 'text'
gt_df = gt_df[['choose_one', 'text']]

# Assign 1 to target column of rows marked 'Relevant' and 0 to others
gt_df['target'] = (gt_df['choose_one'] == 'Relevant').astype(int)

# Add an id to each line / Her satıra bir id ekle
gt_df['id'] = gt_df.index
gt_df

#This code performs operations on a DataFrame and selects the choose_one and text columns from the dataset. It assigns a value of 1 to the target column for rows in the choose_one column with the value "Relevant", and 0 to all other rows. It also adds an id to each row. These operations create a new DataFrame.

merged_df = pd.merge(test_df, gt_df, on='id')
merged_df

subm_df = merged_df[['id', 'target']]
subm_df

subm_df.to_csv('submission_Hanoriega.csv', index=False)
